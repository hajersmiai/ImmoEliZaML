{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4906c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from  sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import lightgbm as lgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_percentage_error, mean_squared_error, r2_score, mean_absolute_error\n",
    "#from  typing import\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy import sparse\n",
    "from lightgbm import early_stopping, log_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff96a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Ensure X is numeric, clean, and has no missing values\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_clean \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      3\u001b[0m X_clean \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# (Optional) Standardize features\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "dX=pd.read_csv(\"data/cleaned_data_after_imputation.csv\")\n",
    "# Ensure X is numeric, clean, and has no missing values\n",
    "X_clean = X.copy()\n",
    "X_clean = X.dropna()\n",
    "\n",
    "# (Optional) Standardize features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_clean)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Get the amount of variance explained by each principal component\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Create a DataFrame for feature contributions to each principal component\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T, \n",
    "    columns=[f'PC{i+1}' for i in range(len(pca.components_))],\n",
    "    index=X_clean.columns\n",
    ")\n",
    "\n",
    "# Sum of absolute loadings to rank original features by their overall impact\n",
    "feature_importance = loadings.abs().sum(axis=1).sort_values(ascending=False)\n",
    "\n",
    "# Display top 20 influential features based on PCA loadings\n",
    "print(feature_importance.head(20))\n",
    "\n",
    "# Plot top N features\n",
    "top_n = 50\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=feature_importance.head(top_n).values, y=feature_importance.head(top_n).index, palette=\"rocket\")\n",
    "plt.title(f\"Top {top_n} Feature Importances from PCA Loadings\")\n",
    "plt.xlabel(\"Sum of Absolute Loadings\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c73008",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=80)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "xgb_regressor = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror', # For regression tasks, specify the objective\n",
    "    n_estimators=100,             # Number of boosting rounds\n",
    "    learning_rate=0.1,            # Step size shrinkage\n",
    "    max_depth=5,                  # Maximum depth of a tree\n",
    "    subsample=0.8,                # Subsample ratio of the training instance\n",
    "    colsample_bytree=0.8,         # Subsample ratio of columns\n",
    "    random_state=42,              # For reproducibility\n",
    "    n_jobs=-1                     # Use all available CPU cores\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "xgb_regressor.fit(X_train_pca, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "xgb_predictions = xgb_regressor.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the XGBoost model\n",
    "xgb_mse = mean_squared_error(y_test, xgb_predictions)\n",
    "xgb_r2 = r2_score(y_test, xgb_predictions)\n",
    "\n",
    "print(f\"XGBoost MSE: {xgb_mse}\")\n",
    "predictions = xgb_regressor.predict(X_test_pca)\n",
    "accu = r2_score(y_test, predictions)\n",
    "print(\"Accuracy of test:\", accu)\n",
    "\n",
    "pred = xgb_regressor.predict(X_train_pca)\n",
    "accu = r2_score(y_train, pred)\n",
    "print(\"Accuracy of train:\", accu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
